{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers in case of version incompatibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall scikit-learn\n",
    "!pip install scikit-learn==1.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If jupyter does not find the isntalled shap version try uninstalling + installing via terminal, restart kernel, then load again\n",
    "!pip uninstall shap\n",
    "!pip install shap==0.46\n",
    "import shap\n",
    "print(shap.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap==0.46\n",
    "import shap\n",
    "print(shap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import *\n",
    "import os\n",
    "class export_ext_val:\n",
    "    def __init__(self,pl) -> None:\n",
    "        self.user_input=pl.user_input\n",
    "        self.master_RFC=pl.master_RFC\n",
    "        self.list_estimators=[i.best_estimator_ for i in pl.master_RFC.models]\n",
    "        self.name=pl.name\n",
    "        self.ohe=pl.ohe\n",
    "        self.mapper=pl.mapper\n",
    "        self.columngroups_df=pl.data.columngroups_df\n",
    "    def save_to(self,path:str ='.'):\n",
    "        save_dir = os.path.join(self.user_input.path, \"Models\", \"Validation_Objects\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        file_path = os.path.join(save_dir, f\"{self.name}_external_val.joblib\")\n",
    "        dump(self, file_path)\n",
    "\n",
    "            # Print confirmation message\n",
    "        print(f\"External validation object has been saved to: {os.path.abspath(file_path)}\")\n",
    "        \n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "plt.rcParams['font.sans-serif'] = ['DejaVu Sans'] #change font to a known standard font"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load All Of us Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "\n",
    "X_file_path = os.path.join(path, \"HCC/X_outer_basic_all.csv\")\n",
    "y_file_path = os.path.join(path, \"HCC/y_outer_basic_all.csv\")\n",
    "\n",
    "# Read the CSV files\n",
    "X_val_df = pd.read_csv(X_file_path)\n",
    "y_val_df = pd.read_csv(y_file_path)\n",
    "X_val_df['split_int'] = 0 #necessary for current pipeline version, because expects split_int\n",
    "\n",
    "# Print info about the loaded dataframes\n",
    "print(f\"X_val_df shape: {X_val_df.shape}\")\n",
    "print(f\"y_val_df shape: {y_val_df.shape}\")\n",
    "\n",
    "# Optionally, display the first few rows of each dataframe\n",
    "print(\"\\nFirst few rows of X_val_df:\")\n",
    "print(X_val_df.head())\n",
    "print(\"\\nFirst few rows of y_val_df:\")\n",
    "print(y_val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.data.y_val[\"status\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset Population for bias investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_df\n",
    "y_val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model oject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Validation_Objects/Pipeline_HCC_all_Model_TOP15_RFC_external_val.joblib'\n",
    "full_path = os.path.join(path, model_name)\n",
    "model_path = []\n",
    "ext_val = load(full_path) # Load the file\n",
    "print(f\"Loading file from: {full_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext={}\n",
    "pl_ext=Pipeline(ext_val_obj=ext_val) #Initialize pipeline object\n",
    "pl_ext.external_validation(X_val=X_val_df,y_val=y_val_df)  # Load ext_data into pipeline\n",
    "\n",
    "# Setup filepaths\n",
    "pl_ext.user_input.path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "pl_ext.user_input.fig_path = pl_ext.user_input.path + \"/HCC/visuals\"\n",
    "pl_ext.pipeline_output_path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "pl_ext.user_input.model_path = pl_ext.pipeline_output_path\n",
    "pl_ext.user_input.target_to_validate_on = \"status\" #Import to change this especially if in training was validated on status_cancerreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply OHE Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.ohe.transform(pl_ext.data.X_val) #Apply one-hot encoder\n",
    "#pl_ext.data.X_ohe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.user_input.model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe before (df does not change for models, so no integration in loop)\n",
    "models = [\"TOP75\", \"TOP30\", \"AMAP-RFC\"]\n",
    "\n",
    "for model in models:\n",
    "    #Load model object\n",
    "    path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "    model_name = f'Validation_Objects/Pipeline_HCC_all_Model_{model}_RFC_external_val.joblib'\n",
    "    full_path = os.path.join(path, model_name)\n",
    "    ext_val = load(full_path) # Load the file\n",
    "    print(f\"Loading file from: {full_path}\")\n",
    "    \n",
    "    \n",
    "    #Initialize pipeline\n",
    "    pl_ext={} #reset pipeline before creating new one\n",
    "    pl_ext=Pipeline(ext_val_obj=ext_val) #Initialize pipeline object\n",
    "    pl_ext.external_validation(X_val=X_val_df,y_val=y_val_df)  # Load ext_data into pipeline\n",
    "\n",
    "    # Setup filepaths\n",
    "    pl_ext.user_input.path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "    pl_ext.user_input.fig_path = pl_ext.user_input.path + \"HCC/visuals\"\n",
    "    pl_ext.pipeline_output_path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "    pl_ext.user_input.model_path = pl_ext.pipeline_output_path\n",
    "    pl_ext.user_input.target_to_validate_on = \"status\"\n",
    "    \n",
    "    #Apply one-hot encoder\n",
    "    pl_ext.ohe.transform(pl_ext.data.X_val) \n",
    "    \n",
    "    # Create master_rfc\n",
    "    pl_ext.build_master_RFC()\n",
    "    \n",
    "    #Initialize eval class\n",
    "    pl_ext.evaluation(only_val=True)\n",
    "    \n",
    "    #Export evaluation\n",
    "    pl_ext.save_values_for_validation()\n",
    "    \n",
    "    #Save Pipeline object for future reference\n",
    "    pl_ext.save_Pipeline()\n",
    "    \n",
    "    \n",
    "print(\"Loop finished\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe before (df does not change for models, so no integration in loop)\n",
    "models = [\"TOP75\", \"TOP30\", \"TOP15\"]\n",
    "\n",
    "for model in models:\n",
    "    #Load model object\n",
    "    path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "    model_name = f'Validation_Objects/Pipeline_HCC_all_Model_{model}_RFC_external_val.joblib'\n",
    "    full_path = os.path.join(path, model_name)\n",
    "    ext_val = load(full_path) # Load the file\n",
    "    print(f\"Loading file from: {full_path}\")\n",
    "    \n",
    "    \n",
    "    #Initialize pipeline\n",
    "    pl_ext={} #reset pipeline before creating new one\n",
    "    pl_ext=Pipeline(ext_val_obj=ext_val) #Initialize pipeline object\n",
    "    pl_ext.external_validation(X_val=X_val_df,y_val=y_val_df)  # Load ext_data into pipeline\n",
    "\n",
    "    # Setup filepaths\n",
    "    pl_ext.user_input.path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "    pl_ext.user_input.fig_path = pl_ext.user_input.path + \"HCC/visuals\"\n",
    "    pl_ext.pipeline_output_path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "    pl_ext.user_input.model_path = pl_ext.pipeline_output_path\n",
    "    pl_ext.user_input.target_to_validate_on = \"status\"\n",
    "    \n",
    "    #Apply one-hot encoder\n",
    "    pl_ext.ohe.transform(pl_ext.data.X_val) \n",
    "    \n",
    "    # Create master_rfc\n",
    "    pl_ext.build_master_RFC()\n",
    "    \n",
    "    #Initialize eval class\n",
    "    pl_ext.evaluation(only_val=True)\n",
    "    pl_ext.evaluation_summary_independent()\n",
    "    pl_ext.evaluation_summary_threshold_dependent(thresholds=np.arange(0.7, 0.29, -0.02), beta=10)\n",
    "\n",
    "    \n",
    "    \n",
    "print(\"Loop finished\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"trained Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.build_master_RFC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.user_input.target_to_validate_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.evaluation(only_val=True) #initialize eval class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model metrics for plots + export pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.save_values_for_validation()\n",
    "pl_ext.save_Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from pprint import pprint\n",
    "\n",
    "def get_excel_metadata(file_path):\n",
    "    \"\"\"\n",
    "    Extract metadata from an Excel file.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to the Excel file\n",
    "    \n",
    "    Returns:\n",
    "    dict: Metadata including number of sheets and column names for each sheet\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        \"file_path\": file_path,\n",
    "        \"number_of_sheets\": 0,\n",
    "        \"sheets\": {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Load the workbook\n",
    "        workbook = openpyxl.load_workbook(file_path, read_only=True)\n",
    "        \n",
    "        # Get the number of sheets\n",
    "        metadata[\"number_of_sheets\"] = len(workbook.sheetnames)\n",
    "        \n",
    "        # Iterate through each sheet\n",
    "        for sheet_name in workbook.sheetnames:\n",
    "            sheet = workbook[sheet_name]\n",
    "            \n",
    "            # Get column names (assuming first row contains headers)\n",
    "            column_names = [cell.value for cell in next(sheet.iter_rows(min_row=1, max_row=1))]\n",
    "            \n",
    "            # Add sheet info to metadata\n",
    "            metadata[\"sheets\"][sheet_name] = {\n",
    "                \"column_names\": column_names,\n",
    "                \"number_of_columns\": len(column_names)\n",
    "            }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# File path\n",
    "file_path = \"/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/combined_output/val/Prediction_values_combined.xlsx\"\n",
    "\n",
    "# Get metadata\n",
    "excel_metadata = get_excel_metadata(file_path)\n",
    "\n",
    "# Print metadata\n",
    "print(\"Excel File Metadata:\")\n",
    "pprint(excel_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create summary tables for model metrics (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def round_columns(df, columns, precision):\n",
    "    df[columns] = df[columns].applymap(lambda x: round(x, precision))\n",
    "    return df\n",
    "\n",
    "\n",
    "columns_precision = {\n",
    "                'Mean': 3,\n",
    "                'Std. Dev.': 3,\n",
    "                'Fold 1': 3,\n",
    "                'Fold 2': 3,\n",
    "                'Fold 3': 3,\n",
    "                'Fold 4': 3,\n",
    "                'Fold 5': 3\n",
    "            }\n",
    "\n",
    "all_evaluation_results = pd.DataFrame()  # For storing all evals\n",
    "row_subsets = [\"par\", \"all\"]\n",
    "col_subsets = ['Model_A', 'Model_B', 'Model_C', 'Model_D', 'Model_E']\n",
    "\n",
    "for row_subset in row_subsets:\n",
    "    for col_subset in col_subsets:\n",
    "        pl = load_Pipeline(path+ f'/Models/Pipelines/RFC/Pipeline_HCC_{row_subset}_{col_subset}_RFC.joblib')\n",
    "        pl.evaluation()\n",
    "        aucs = pl.eval.val[\"aucs\"]\n",
    "        auprcs = pl.eval.val[\"auprcs\"]\n",
    "        mean_auc = np.mean(aucs)\n",
    "        std_auc = np.std(aucs)\n",
    "        mean_auprc = np.mean(auprcs)\n",
    "        std_auprc = np.std(auprcs)\n",
    "\n",
    "        # Create evaluation table for AUROC\n",
    "        evaluation_table_auroc = pd.DataFrame({\n",
    "            'Model': [col_subset],\n",
    "            'Dataset': [row_subset],\n",
    "            'Metric': ['AUROC'],\n",
    "            'Mean': [mean_auc],\n",
    "            'Std. Dev.': [std_auc],\n",
    "            'Fold 1': [aucs[0]],\n",
    "            'Fold 2': [aucs[1]],\n",
    "            'Fold 3': [aucs[2]],\n",
    "            'Fold 4': [aucs[3]],\n",
    "            'Fold 5': [aucs[4]]\n",
    "\n",
    "        })\n",
    "\n",
    "        # Create evaluation table for AUPRC\n",
    "        evaluation_table_auprc = pd.DataFrame({\n",
    "            'Model': [col_subset],\n",
    "            'Dataset': [row_subset],\n",
    "            'Metric': ['AUPRC'],\n",
    "            'Mean': [mean_auprc],\n",
    "            'Std. Dev.': [std_auprc],\n",
    "            'Fold 1': [auprcs[0]],\n",
    "            'Fold 2': [auprcs[1]],\n",
    "            'Fold 3': [auprcs[2]],\n",
    "            'Fold 4': [auprcs[3]],\n",
    "            'Fold 5': [auprcs[4]]\n",
    "\n",
    "        })\n",
    "\n",
    "        # Concatenate the results\n",
    "        all_evaluation_results = pd.concat([all_evaluation_results, evaluation_table_auroc, evaluation_table_auprc], ignore_index=True)\n",
    "\n",
    "        for column, precision in columns_precision.items():\n",
    "                all_evaluation_results = round_columns(all_evaluation_results, [column], precision)\n",
    "\n",
    "print(all_evaluation_results)\n",
    "\n",
    "output_path = path+ f'/Models/Pipelines/RFC/combined_output/val/all_evaluation_results.xlsx'\n",
    "with pd.ExcelWriter(output_path, mode='a') as writer:\n",
    "    all_evaluation_results.to_excel(writer, sheet_name='Independent metrics', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Summary model metrics (threshold-dependent) (2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, average_precision_score, precision_score, recall_score, balanced_accuracy_score, confusion_matrix, fbeta_score\n",
    "\n",
    "def round_columns(df, columns, precision):\n",
    "    df[columns] = df[columns].applymap(lambda x: round(x, precision))\n",
    "    return df\n",
    "\n",
    "columns_precision = {\n",
    "                'Precision': 3,\n",
    "                'Recall': 2,\n",
    "                'Accuracy': 2,\n",
    "                'F1 Score': 3,\n",
    "                'F-beta Score': 3,\n",
    "                'Balanced Accuracy': 2,\n",
    "                'PPV': 4,\n",
    "                'NPV': 4\n",
    "            }\n",
    "row_subsets = [\"par\", \"all\"]\n",
    "col_subsets = ['Model_C']\n",
    "beta = 10\n",
    "\n",
    "\n",
    "thresholds = np.arange(0.6, 0.29, -0.01)\n",
    "threshold_evaluation_results = pd.DataFrame()\n",
    "\n",
    "for row_subset in row_subsets:\n",
    "    for col_subset in col_subsets:\n",
    "        pl = load_Pipeline(path + f'/Models/Pipelines/RFC/Pipeline_HCC_{row_subset}_{col_subset}_RFC.joblib')\n",
    "        pl.evaluation()\n",
    "\n",
    "        for threshold in thresholds:\n",
    "\n",
    "            proba = pl.master_RFC.predict_proba(pl.ohe.transform(pl.data.X_val)) # Predictions at the given threshold\n",
    "\n",
    "\n",
    "            print(f\"Prediction probabilities (first 5): {proba[:5]}\") # Debug print\n",
    "\n",
    "            if proba.ndim == 1:\n",
    "                y_pred = (proba >= threshold).astype(int)\n",
    "            else:\n",
    "                y_pred = (proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "            y_true = pl.data.z_val[\"status_cancerreg\"]\n",
    "\n",
    "\n",
    "            # Calculate metrics\n",
    "            precision = precision_score(y_true, y_pred)\n",
    "            recall = recall_score(y_true, y_pred)\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            f_beta = fbeta_score(y_true, y_pred, beta=beta)\n",
    "            balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "            ppv = tp / (tp + fp)  # Same as precision\n",
    "            npv = tn / (tn + fn)\n",
    "\n",
    "            # Create evaluation table for the threshold\n",
    "            evaluation_table_threshold = pd.DataFrame({\n",
    "                'Model': [col_subset],\n",
    "                'Dataset': [row_subset],\n",
    "                'Threshold': [threshold],\n",
    "                'Precision': [precision],\n",
    "                'Recall': [recall],\n",
    "                'Accuracy': [accuracy],\n",
    "                'F1 Score': [f1],\n",
    "                f'F-beta Score (beta={beta})': [f_beta],\n",
    "                'Balanced Accuracy': [balanced_accuracy],\n",
    "                'PPV': [ppv],\n",
    "                'NPV': [npv]\n",
    "            })\n",
    "\n",
    "            # Concatenate the results\n",
    "            threshold_evaluation_results = pd.concat([threshold_evaluation_results, evaluation_table_threshold], ignore_index=True)\n",
    "\n",
    "            for column, precision in columns_precision.items():\n",
    "                threshold_evaluation_results = round_columns(threshold_evaluation_results, [column], precision)\n",
    "\n",
    "\n",
    "output_path = path+ f'/Models/Pipelines/RFC/combined_output/val/all_evaluation_results.xlsx'\n",
    "with pd.ExcelWriter(output_path, mode='a') as writer: #append second sheet (a). If exporting this first, use (w)\n",
    "    threshold_evaluation_results.to_excel(writer, sheet_name='Threshold metrics_all_thresholds', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import * #Load our package with classes pipeline, models, pp (preprocessing), plot, and more\n",
    "path = '/home/jupyter/workspaces/machinelearningforlivercancerriskprediction/'\n",
    "pl_ext=load_Pipeline(path + \"/Pipelines/not_trained/Pipeline_HCC_all_Model_TOP15_RFC.joblib\") #Change for pipeline you want\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Ethnicity info for subanalysis\n",
    "\n",
    "df_ethnicity_boolean = pd.read_csv(os.path.join(path, \"HCC/df_ethnicity_boolean.csv\"))\n",
    "pl_ext.data.X_val = pl_ext.data.X_val.merge(df_ethnicity_boolean, on=\"eid\", how=\"inner\")\n",
    "\n",
    "# Print info about the resulting DataFrame\n",
    "print(pl_ext.data.X_val.shape)\n",
    "print(pl_ext.data.X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append timepoint of diagnosis for Kaplan Meier curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.user_input.fig_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.feature_imp_barplot(n_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.shap_analysis(sample_size=10000, max_display=15, fig_size=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.data.z_val.status_cancerreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_ext.roc_auc_test_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save_colorbar(pip_self=pl, figsize=(0.5, 6), font_size=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.wrapper_eval_prediction_mono(pip_self=pl_ext,X=pl_ext.data.X_val,y_true=pl_ext.data.y_val[\"status\"],model=pl_ext.master_RFC,thresholds=[0.55, 0.45, 0.35, 0.6, 0.5, 0.4],figsize=(15,10), font_size=22, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.wrapper_eval_prediction_multi(pip_self=pl_ext,X=pl_ext.data.X_val,y_true=pl_ext.data.y_val[\"status\"],model=pl_ext.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.create_violin_plot(pl_ext,pl_ext.data.X_val,pl_ext.data.y_val,model=pl_ext.master_RFC,ohe=pl_ext.ohe, gap=-0.1, width=0.8, thresholds_choice=[0,.35,.55,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_KM(pl,thresholds=(0.35, 0.5),color_dict={'Low Risk':'green','Medium Risk':'yellow','High Risk':'red'}, x_scale=\"y\", y_scale=\"default\", font_size=\"22\"):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from lifelines import KaplanMeierFitter\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def kaplan_meier_analysis(time, event, group, x_scale= x_scale, group_labels=None, plot=True,color_dict=color_dict, font_size=font_size, y_scale=y_scale):\n",
    "        \"\"\"\n",
    "        Perform Kaplan-Meier analysis and compare survival curves between two groups.,\n",
    "        Parameters:,\n",
    "        - time: Array-like object containing the time to event or censoring.,\n",
    "        - event: Array-like object indicating whether an event occurred (1) or not (0).,\n",
    "        - group: Array-like object specifying the group each observation belongs to (e.g., treatmentcontrol).,\n",
    "        - group_labels: List of labels for the two groups (optional).,\n",
    "         -x-scale (y, m, d)\n",
    "        - plot: Boolean indicating whether to plot the survival curves (default is True).,\n",
    "        Returns:,\n",
    "        - kmf: KaplanMeierFitter object containing the survival estimates for each group.,\n",
    "        \"\"\"\n",
    "        # Creating a DataFrame from the provided data,\n",
    "        df = pd.DataFrame({'time': list(time), 'event': list(event), 'group': list(group)})\n",
    "        # Initializing KaplanMeierFitter,\n",
    "        kmf = KaplanMeierFitter()\n",
    "        # Group-wise analysis,\n",
    "        for i, grp in enumerate([i for i in df['group'].unique()]):\n",
    "            data = df.loc[df['group'] == grp,:]\n",
    "            kmf.fit(data['time'], event_observed=data['event'], label=group_labels[i] if group_labels else f'{grp}')\n",
    "            if plot:\n",
    "                kmf.plot(color=color_dict.get(grp,'green'),alpha=0.7)\n",
    "\n",
    "        if plot:\n",
    "            if x_scale == 'y':\n",
    "                plt.xlabel('Time [Years]', fontsize=font_size)\n",
    "            elif x_scale == 'm':\n",
    "                plt.xlabel('Time [Months]', fontsize=font_size)\n",
    "            else:\n",
    "                plt.xlabel('Time [Days]', fontsize=font_size)\n",
    "            plt.ylabel(f'1 - Probability of {pl.user_input.DOI} [%]', fontsize=font_size),\n",
    "            plt.yticks(fontsize=font_size)\n",
    "            plt.xticks(fontsize=font_size)\n",
    "            plt.title(f'Time to {pl.user_input.DOI} per risk group'),\n",
    "            plt.legend(frameon=False, fontsize=font_size, loc=\"lower left\"),\n",
    "            if y_scale != 'default':\n",
    "                plt.ylim(y_scale)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        return kmf\n",
    "\n",
    "    def get_group(pred_prob,thresholds):\n",
    "        groups=[]\n",
    "        for i in pred_prob:\n",
    "            if i <thresholds[0]:\n",
    "                groups.append('Low Risk')\n",
    "            elif i>=thresholds[0] and i<thresholds[1]:\n",
    "                groups.append('Medium Risk')\n",
    "            elif i>=thresholds[1]:\n",
    "                groups.append('High Risk')\n",
    "        return groups\n",
    "\n",
    "    time_censoring=pd.Timestamp(year=2024,day=1,month=1)\n",
    "    time_censoring\n",
    "    z_val=pl.data.z_val\n",
    "\n",
    "\n",
    "    z_val.loc[z_val.date_of_diag.isna(),'date_of_diag']=time_censoring\n",
    "    timedelta=pd.to_datetime(z_val.date_of_diag)-(pd.to_datetime(z_val['Date of assessment']))\n",
    "    z_val['time_to_event_d']= [i.days for i in timedelta]\n",
    "    z_val['time_to_event_m']= z_val['time_to_event_d'] /30\n",
    "    z_val['time_to_event_y']= z_val['time_to_event_d'] /365.25\n",
    "    z_val[\"pred_prob\"]=pl.master_RFC.predict_proba(pl.ohe.transform(pl.data.X_val)).values\n",
    "\n",
    "\n",
    "    z_val['risk_group']=get_group(z_val.pred_prob,thresholds=thresholds)\n",
    "    print('The Thresholds are:',thresholds)\n",
    "    print(z_val.risk_group.value_counts())\n",
    "    fig,ax=plt.subplots(figsize=(10,10))\n",
    "    time_column = f'time_to_event_{x_scale}'\n",
    "    estimator=kaplan_meier_analysis(z_val[time_column], event=z_val.status,group=z_val.risk_group)\n",
    "    estimator.plot_survival_function()\n",
    "\n",
    "    svg_path = os.path.join(pl.user_input.fig_path, f\"KaplanMeier_{pl.user_input.col_subset}_{pl.user_input.row_subset}_{y_scale}.svg\")\n",
    "    fig.savefig(svg_path, format='svg', bbox_inches='tight', transparent=True)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf Matrices Male\n",
    "plot.wrapper_eval_prediction_multi(pip_self=pl_ext,X=pl_ext.data.X_val,y_true=pl_ext.data.y_val[\"status\"],model=pl_ext.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'SEX', 'value': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf Matrices Male\n",
    "plot.wrapper_eval_prediction_multi(pip_self=pl_ext,X=pl_ext.data.X_val,y_true=pl_ext.data.y_val[\"status\"],model=pl_ext.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'SEX', 'value': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf Matrices White\n",
    "plot.wrapper_eval_prediction_multi(pip_self=pl_ext,X=pl_ext.data.X_val,y_true=pl_ext.data.y_val[\"status\"],model=pl_ext.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'race_binary', 'value': \"White\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conf Matrices White\n",
    "plot.wrapper_eval_prediction_multi(pip_self=pl_ext,X=pl_ext.data.X_val,y_true=pl_ext.data.y_val[\"status\"],model=pl_ext.master_RFC,thresholds=[(0.4, 0.6), (0.35, 0.55)],incorp_threh_in_y_label=True,figsize=(13,5), n_rows=1, font_size=22, stratify={'column': 'race_binary', 'value': \"Non-White\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small info prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_KM(pl, thresholds=(0.35, 0.6), x_scale='y', font_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_KM(pl, thresholds=(0.35, 0.55), x_scale='y', y_scale=(0,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
